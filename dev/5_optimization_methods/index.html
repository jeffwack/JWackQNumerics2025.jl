<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Optimization Methods · QNumerics2025.jl</title><meta name="title" content="Optimization Methods · QNumerics2025.jl"/><meta property="og:title" content="Optimization Methods · QNumerics2025.jl"/><meta property="twitter:title" content="Optimization Methods · QNumerics2025.jl"/><meta name="description" content="Documentation for QNumerics2025.jl."/><meta property="og:description" content="Documentation for QNumerics2025.jl."/><meta property="twitter:description" content="Documentation for QNumerics2025.jl."/><meta property="og:url" content="https://jeffwack.github.io/QNumerics2025.jl/5_optimization_methods/"/><meta property="twitter:url" content="https://jeffwack.github.io/QNumerics2025.jl/5_optimization_methods/"/><link rel="canonical" href="https://jeffwack.github.io/QNumerics2025.jl/5_optimization_methods/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">QNumerics2025.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../creation/">Creating this package</a></li><li><a class="tocitem" href="../1_intro_to_julia/">Intro to Julia</a></li><li><a class="tocitem" href="../2_introduction_to_state_vector_simulation/">Introduction to State Vector Simulation</a></li><li><a class="tocitem" href="../3_dynamics_and_quantumoptics/">Dynamics and QuantumOptics.jl</a></li><li><a class="tocitem" href="../4_quantum_trajectories/">Quantum Trajectories</a></li><li class="is-active"><a class="tocitem" href>Optimization Methods</a><ul class="internal"><li><a class="tocitem" href="#Getting-started"><span>Getting started</span></a></li><li><a class="tocitem" href="#Unconstrained-Optimization"><span>Unconstrained Optimization</span></a></li><li><a class="tocitem" href="#Newton&#39;s-Method"><span>Newton&#39;s Method</span></a></li><li><a class="tocitem" href="#Constrained-Optimization"><span>Constrained Optimization</span></a></li></ul></li><li><a class="tocitem" href="../6_grape_demos/">GRAPE Demos</a></li><li><a class="tocitem" href="../7_trajectory_optimization/">Trajectory Optimization</a></li><li><a class="tocitem" href="../8_nonlinear_trajectory_optimization/">Nonlinear Trajectory Optimization</a></li><li><a class="tocitem" href="../9_quantum_trajectory_optimization/">Quantum Trajectory Optimization</a></li><li><a class="tocitem" href="../10_clifford/">Clifford</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Optimization Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Optimization Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/jeffwack/QNumerics2025.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/jeffwack/QNumerics2025.jl/blob/main/docs/src/5_optimization_methods.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Optimization-Methods-Lecture-1"><a class="docs-heading-anchor" href="#Optimization-Methods-Lecture-1">Optimization Methods - Lecture 1</a><a id="Optimization-Methods-Lecture-1-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-Methods-Lecture-1" title="Permalink"></a></h1><h2 id="Getting-started"><a class="docs-heading-anchor" href="#Getting-started">Getting started</a><a id="Getting-started-1"></a><a class="docs-heading-anchor-permalink" href="#Getting-started" title="Permalink"></a></h2><h3 id="Goals"><a class="docs-heading-anchor" href="#Goals">Goals</a><a id="Goals-1"></a><a class="docs-heading-anchor-permalink" href="#Goals" title="Permalink"></a></h3><ul><li>Gradient descent</li><li>Newton&#39;s method</li><li>Regularization</li><li>Newton approximations</li><li>Line search</li><li>KKT conditions</li><li>Exercises with GRAPE</li></ul><h2 id="Unconstrained-Optimization"><a class="docs-heading-anchor" href="#Unconstrained-Optimization">Unconstrained Optimization</a><a id="Unconstrained-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Unconstrained-Optimization" title="Permalink"></a></h2><p class="math-container">\[\min_{\bm x} J(\bm x)\]</p><h3 id="Necessary-Conditions"><a class="docs-heading-anchor" href="#Necessary-Conditions">Necessary Conditions</a><a id="Necessary-Conditions-1"></a><a class="docs-heading-anchor-permalink" href="#Necessary-Conditions" title="Permalink"></a></h3><p class="math-container">\[\nabla J ( \bm{x}^* ) = \bm{0}\]</p><p class="math-container">\[\nabla^2 J(\bm{x}^*) \succeq \bm{0}\]</p><h3 id="Gradient-Descent"><a class="docs-heading-anchor" href="#Gradient-Descent">Gradient Descent</a><a id="Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent" title="Permalink"></a></h3><h4 id="Gradient-Descent-Update-Rule"><a class="docs-heading-anchor" href="#Gradient-Descent-Update-Rule">Gradient Descent Update Rule</a><a id="Gradient-Descent-Update-Rule-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-Descent-Update-Rule" title="Permalink"></a></h4><p>The gradient descent algorithm updates the current point <span>$x$</span> by stepping in the direction opposite the gradient:</p><p class="math-container">\[\bm{x}_{i+1} = \bm{x}_{i} - \eta \cdot \nabla J(\bm{x}_{i})\]</p><p>where <span>$\eta$</span> is the learning rate - controlling the step size.</p><h4 id="Example-solve"><a class="docs-heading-anchor" href="#Example-solve">Example solve</a><a id="Example-solve-1"></a><a class="docs-heading-anchor-permalink" href="#Example-solve" title="Permalink"></a></h4><p>We&#39;re going to apply <strong>gradient descent</strong> to minimize the following:</p><p class="math-container">\[h(x) = x^4 + x^3 - x^2 - x\]</p><p>This is a smooth, non-linear function with multiple stationary points. Our goal is to find a local minimum by starting from an initial value and following the gradient downhill: <span>$\nabla h(x^*) = 0$</span></p><pre><code class="language-julia hljs">using CairoMakie

function h(x)
    return x.^4 + x.^3 - x.^2 - x
end

function ∇h(x)
    return 4.0*x.^3 + 3.0*x.^2 - 2.0*x - 1.0
end

x = range(-1.75,1.25,1000)

# Initial guess
x₀ = 1.19

xᵢ = x₀

gradient_descent_step(xᵢ; η=0.01) = xᵢ - η * ∇h(xᵢ)

# Initial plot
fig1 = Figure()
ax1 = Axis(fig1[1,1])
lines!(ax1, x, h(x))

# Perform gradient descent step
xᵢ₊₁ = gradient_descent_step(xᵢ, η=0.01)
plot!(ax1, [xᵢ], [h(xᵢ)], color=:orange, marker=&#39;x&#39;, markersize=25)
xᵢ = xᵢ₊₁
fig1</code></pre><img src="8b37bb55.png" alt="Example block output"/><h2 id="Newton&#39;s-Method"><a class="docs-heading-anchor" href="#Newton&#39;s-Method">Newton&#39;s Method</a><a id="Newton&#39;s-Method-1"></a><a class="docs-heading-anchor-permalink" href="#Newton&#39;s-Method" title="Permalink"></a></h2><p>Now we&#39;re using a local quadratic approximation, for a nearby guess <span>$x_k$</span> the value of <span>$J(\bm{x})$</span> can be well-approximated as</p><p class="math-container">\[J(\bm{x}) \approx J(\bm{x}_k) + \nabla J(\bm{x})^T (\bm{x} - \bm{x}_k) + \frac{1}{2} (\bm{x} - \bm{x}_k)^T \nabla^2 J(\bm{x}_k) (\bm{x} - \bm{x}_k)\]</p><p>And we can solve for our necessary condition</p><p class="math-container">\[\begin{align}
\bm{0} &amp; \overset{\mathrm{!}}{=} \nabla J(\bm{x} + \Delta \bm{x}) \approx \nabla J(\bm{x}) + \nabla^2 J(\bm{x}) \Delta \bm{x} \\
\Delta \bm{x} &amp; \overset{\mathrm{!}}{=} - \left(\nabla^2 J(\bm{x}) \right)^{-1} \nabla J(\bm{x}) \\ 
\end{align}\]</p><pre><code class="language-julia hljs">function ∇²h(x)
    return 12.0*x.^2 + 6.0*x - 2.0
end

xᵢ = 0.0
newton_step(xᵢ) = xᵢ - ∇²h(xᵢ)\∇h(xᵢ)

# Initial plot
fig2 = Figure()
ax2 = Axis(fig2[1,1])
lines!(ax2, x, h(x))

xᵢ₊₁ = newton_step(xᵢ)
plot!(ax2, [xᵢ], [h(xᵢ)], color=:orange, marker=&#39;x&#39;, markersize=25)
xᵢ = xᵢ₊₁
fig2</code></pre><img src="3fa55f5c.png" alt="Example block output"/><h3 id="Add-Regularization"><a class="docs-heading-anchor" href="#Add-Regularization">Add Regularization</a><a id="Add-Regularization-1"></a><a class="docs-heading-anchor-permalink" href="#Add-Regularization" title="Permalink"></a></h3><p>In the last example, our initialization violated our assumption of a positive semi-definite hessian</p><p class="math-container">\[\nabla^2 h(0) = -2\]</p><p>The first trick we can employ is regularization. The intuition is that we want to retain second order approximation of the function for better convergence, but the approximation must ensure that the direction of travel is still one of guaranteed descent.</p><p>Check out some of <a href="https://kenndanielso.github.io/mlrefined/blog_posts/7_Second_order_methods/7_4_Newtons_regularized.html">these visualizations</a> for some of the details and play around with the regularization parameter.</p><p>As mentioned on that page, solving for the regularization parameter is impractical: guess and check approximations to this value can be sufficient and are easily computable.</p><pre><code class="language-julia hljs">using LinearAlgebra

β = 1.0

function regularized_newton_step(xᵢ; β=β)
    H = ∇²h(xᵢ)
    while !isposdef(H) # don&#39;t do this in your own code!
        H = H + β*I
    end
    return xᵢ - H\∇h(xᵢ)
end

# Initial guess
xᵢ = 0.0

# Initial plot
fig3 = Figure()
ax3 = Axis(fig3[1,1])
lines!(ax3, x, h(x))

xᵢ₊₁ = regularized_newton_step(xᵢ)
plot!(ax3, [xᵢ], [h(xᵢ)], color=:orange, marker=&#39;x&#39;, markersize=25)
xᵢ = xᵢ₊₁
fig3</code></pre><img src="3fa55f5c-001.png" alt="Example block output"/><h4 id="How-can-this-go-wrong?"><a class="docs-heading-anchor" href="#How-can-this-go-wrong?">How can this go wrong?</a><a id="How-can-this-go-wrong?-1"></a><a class="docs-heading-anchor-permalink" href="#How-can-this-go-wrong?" title="Permalink"></a></h4><p>For the regularization, we chose <span>$\beta = 1.0$</span> which worked well, but adding <span>$\beta * \bm{I}$</span> to our hessian until it is positive semi-definite could be bad. If the hessian has large negative eigenvalues, it can take a <em>REALLY</em> long time - linear in the magnitude of the smallest eigenvalue.</p><p>In practice, we use additional information to choose a good <span>$\beta$</span>, maybe guessing and checking a few times, and then regularize the quadratic - ideally in just a few guesses and checks.</p><p>If we change the regularization parameter to something too high, this will have convergence no better than gradient descent. If it was too low (but the hessian was still positive semi-definite) we also observed overshoot.</p><p><strong>Final note:</strong> regularization like this helps make the problem &#39;convex&#39; in a way - another benefit to reap is that near flat areas, regularization can help ensure that the hessian does not become ill-conditioned.</p><pre><code class="language-julia hljs">function h(x)
    return 0.125 * x.^4 + sin.(3 * x) .+ 0.5  * x
end

function ∇h(x)
    return 0.5 * x.^3 + 3 * cos.(3 * x) .+ 0.5
end

function ∇²h(x)
    return 1.5 * x.^2 - 9 * sin.(3 * x)
end

x = range(-3.75,3.25,1000)

# Initial guess
x₀ = 0.0

xᵢ = x₀

# Initial plot
fig4 = Figure()
ax4 = Axis(fig4[1,1])
lines!(ax4, x, h(x))

xᵢ₊₁ = regularized_newton_step(xᵢ; β=0.85)
plot!(ax4, [xᵢ], [h(xᵢ)], color=:orange, marker=&#39;x&#39;, markersize=25)
xᵢ = xᵢ₊₁
fig4</code></pre><img src="75a00d5e.png" alt="Example block output"/><h3 id="Add-line-search"><a class="docs-heading-anchor" href="#Add-line-search">Add line search</a><a id="Add-line-search-1"></a><a class="docs-heading-anchor-permalink" href="#Add-line-search" title="Permalink"></a></h3><p>Ok so that was bad, things went sideways and we were all over the place. This function was tailored to diverge with <span>$\beta = 0.85$</span>. We want to make sure when we are taking a step, that the slope is not changing too quickly over the step. We want to take a step that sufficiently decreases the objective</p><pre><code class="language-julia hljs">function backtracking_regularized_newton_step(xᵢ)
    H = ∇²h(xᵢ)

    # regularization
    β = 1.0
    while !isposdef(H)
        H = H + β*I
    end
    Δx = -H\∇h(xᵢ)

    # line search
    b = 0.1
    c = 0.25
    α = 1.0
    while h(xᵢ + α*Δx) &gt; h(xᵢ) + b*α*∇h(xᵢ)*Δx
        α = c*α
    end

    return xᵢ + α*Δx
end

# Initial guess
xᵢ = 0.0

# Initial plot
fig5 = Figure()
ax5 = Axis(fig5[1,1])
lines!(ax5, x, h(x))

xᵢ₊₁ = backtracking_regularized_newton_step(xᵢ)
plot!(ax5, [xᵢ], [h(xᵢ)], color=:green, marker=&#39;x&#39;, markersize=25)
xᵢ = xᵢ₊₁
fig5</code></pre><img src="32db094d.png" alt="Example block output"/><h2 id="Constrained-Optimization"><a class="docs-heading-anchor" href="#Constrained-Optimization">Constrained Optimization</a><a id="Constrained-Optimization-1"></a><a class="docs-heading-anchor-permalink" href="#Constrained-Optimization" title="Permalink"></a></h2><p>In this section, we are going to learn about <em>constrained optimization</em>. Let&#39;s keep things simple, and start without all of the regularization and line search stuff. </p><p>In the next cells, we&#39;ll define a 2D bowl as our cost function, and we&#39;ll draw some nice level curves to visualize it–it&#39;s a convex cost, so we know it will have a minimum at the bottom of the bowl. To make it interesting, we will add a single constraint, which we draw as a curve.</p><p class="math-container">\[J(x, y) = \frac{1}{2} \left( \tfrac{1}{2} (x - 1)^2 + (y - 1)^2 \right)\]</p><p>When constructing our system, to get to a minimum while respecting the constraints, we are using an augmented lagrangian. At each optimization step, we will enforce the constraints - more on this in a moment!</p><pre><code class="language-julia hljs">using ForwardDiff

Q = Diagonal([0.5; 1])

# Objective
function J(x)
    return 1 / 2 * (x - [1; 0])&#39; * Q * (x - [1; 0])
end

function ∇J(x)
    return Q * (x - [1; 0])
end

function ∇²J(x)
    return Q
end

# Nonlinear constraint
function f(x)
    return x[1]^2 + 2*x[1] - x[2]
end

function ∂f(x)
    return [2*x[1]+2 -1]
end

function draw_contour(ax; samples=40, levels=25)
    cols = kron(ones(samples), range(-4, 4, samples)&#39;)
    rows = kron(ones(samples)&#39;, range(-4, 4, samples))
    vals = zeros(samples,samples)
    for j = 1:samples
        for k = 1:samples
            vals[j, k] = J([cols[j, k]; rows[j, k]])
        end
    end
    contour!(ax, vec(cols), vec(rows), vec(vals), levels=levels)

    # Nonlinear x^2 + 2x - y = 0
    constraint = range(-3.2, 1.2, samples)
    lines!(ax, constraint, constraint.^2 .+ 2*constraint, color=:black, linewidth=2)
end

function newton_step(xᵢ, λᵢ)
    ∂²L_∂x² = ∇²J(xᵢ) + ForwardDiff.jacobian(x -&gt; ∂f(x)&#39;λᵢ, xᵢ)
    ∂f_∂x = ∂f(xᵢ)

    # KKT system
    H = [∂²L_∂x² ∂f_∂x&#39;; ∂f_∂x 0]
    g = [∇J(xᵢ) + ∂f_∂x&#39;λᵢ; f(xᵢ)]

    Δz = -H\g
    Δx = Δz[1:2]
    Δλ = Δz[3]
    return xᵢ .+ Δx, λᵢ .+ Δλ
end

fig6 = Figure()
ax6 = Axis(fig6[1,1], aspect=1)

# Initial guess
xᵢ = Float64[-3; 2]
λᵢ = Float64[0.0]

# Draw the initial contours and the initial guess
draw_contour(ax6)
plot!(ax6, [xᵢ[1]], [xᵢ[2]], color=:red, marker=:circle, markersize=15)

# Perform Newton step
xᵢ₊₁, λᵢ₊₁ = newton_step(xᵢ, λᵢ)
plot!(ax6, [xᵢ₊₁[1]], [xᵢ₊₁[2]], color=:red, marker=:x, markersize=15)
xᵢ .= xᵢ₊₁
λᵢ .= λᵢ₊₁

fig6</code></pre><img src="dc7f870b.png" alt="Example block output"/><h3 id="Let&#39;s-talk-about-KKT-systems"><a class="docs-heading-anchor" href="#Let&#39;s-talk-about-KKT-systems">Let&#39;s talk about KKT systems</a><a id="Let&#39;s-talk-about-KKT-systems-1"></a><a class="docs-heading-anchor-permalink" href="#Let&#39;s-talk-about-KKT-systems" title="Permalink"></a></h3><p>We need regularization... even though we picked a convex cost! The constraint in our system makes this problem not convex anymore. Let&#39;s add regularization, but we will do so a bit differently.</p><p><strong>ASIDE</strong> Not in this talk, but that second term has other methods that can be used to approximate it (like LBFGS). LBFGS in particular has some robustness properties that baseline Newton&#39;s method lacks. We will make a similar approximation to our KKT system, called the <em>Gauss-Newton approximation</em>. See the exercises below for a more detailed explanation.</p><p>The thought process is as follows: After inspecting <code>∂²L_∂x²</code>, we note that <span>$\nabla^2 J$</span> is convex by construction. It is just the <code>ForwardDiff.jacobian(x -&gt; ∂f(x)&#39;λᵢ, xᵢ)</code> that causes trouble with the Hessian. At this time, we also notice that latter term is also expensive to compute. Because it causes trouble and is costly to compute, we decide to drop this term. This is the Gauss-Newton approximation. Its steps compute faster, but converge slower than Newton–luckily, the savings in compute speed often overtake any reduction in convergence rate!</p><pre><code class="language-julia hljs">function gauss_newton_step(xᵢ, λᵢ)
    # Implicit regularization - drop the second-order constraint term
    ∂²L_∂x² = ∇²J(xᵢ)
    ∂f_∂x = ∂f(xᵢ)

    # KKT system
    H = [∂²L_∂x² ∂f_∂x&#39;; ∂f_∂x 0]
    g = [∇J(xᵢ) + ∂f_∂x&#39;λᵢ; f(xᵢ)]

    Δz = -H\g
    Δx = Δz[1:2]
    Δλ = Δz[3]
    return xᵢ .+ Δx, λᵢ .+ Δλ
end

fig7 = Figure()
ax7 = Axis(fig7[1,1], aspect=1)

# Initial guess
xᵢ = Float64[-3; 2]
λᵢ = Float64[0.0]

draw_contour(ax7)
plot!(ax7, [xᵢ[1]], [xᵢ[2]], color=:green, marker=:circle, markersize=15)

# Perform Gauss-Newton step
xᵢ₊₁, λᵢ₊₁ = gauss_newton_step(xᵢ, λᵢ)
plot!(ax7, [xᵢ₊₁[1]], [xᵢ₊₁[2]], color=:green, marker=:x, markersize=15)
xᵢ .= xᵢ₊₁
λᵢ .= λᵢ₊₁

fig7</code></pre><img src="0505f6e3.png" alt="Example block output"/><h3 id="The-Gauss-Newton-approximation"><a class="docs-heading-anchor" href="#The-Gauss-Newton-approximation">The Gauss-Newton approximation</a><a id="The-Gauss-Newton-approximation-1"></a><a class="docs-heading-anchor-permalink" href="#The-Gauss-Newton-approximation" title="Permalink"></a></h3><p>This quick calculation should hopefully remind you about the Gauss-Newton approximation.</p><p>Start with a cost <span>$J(\mathbf{x})$</span>. The necessary condition for optimality is <span>$\nabla J(\mathbf{x}) = 0$</span>. Our journey starts by asking what happens if <span>$J(\mathbf{x})$</span> is actually a least squares problem. For example, <span>$J(\mathbf{x}) := \frac{1}{2}||\mathbf{g}(\mathbf{x})||_2^2$</span>. </p><p class="math-container">\[\nabla J(\mathbf{x}) = \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}^\text{T} \mathbf{g}(\mathbf{x})\]</p><p class="math-container">\[\nabla^2 J(\mathbf{x}) = \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}^\text{T} \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}} + \nabla\left(\frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}^\text{T}\right) \mathbf{g}(\mathbf{x}) \approx \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}^\text{T} \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}\]</p><p class="math-container">\[\Delta \mathbf{x} = - \left(\frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}^\text{T} \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}\right)^{-1}
    \frac{\partial \mathbf{g}(\mathbf{x})}{\partial \mathbf{x}}^\text{T} \mathbf{g}(\mathbf{x})\]</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../4_quantum_trajectories/">« Quantum Trajectories</a><a class="docs-footer-nextpage" href="../6_grape_demos/">GRAPE Demos »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.14.1 on <span class="colophon-date" title="Friday 18 July 2025 02:44">Friday 18 July 2025</span>. Using Julia version 1.11.6.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
