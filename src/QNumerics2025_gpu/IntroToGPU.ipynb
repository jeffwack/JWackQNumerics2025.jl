{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abf20c80",
      "metadata": {
        "id": "abf20c80"
      },
      "source": [
        "# Hands-on with CUDA.jl\n",
        "\n",
        "In this notebook, we'll use [`CUDA.jl`](https://github.com/JuliaGPU/CUDA.jl) to control and program NVIDIA GPU resources. If you have an NVIDIA GPU already (in your laptop or desktop), you can run this notebook locally. If not, you can use a GPU available through [Google Colab](https://colab.research.google.com/). This will allow you to use a GPU *for free* for up to 12 hours of interactive experimentation.\n",
        "\n",
        "We'll use `CUDA.jl` because:\n",
        "- NVIDIA GPUs are highly available\n",
        "- CUDA is (for now) the most mature GPU programming paradigm with many libraries and resources\n",
        "- Many of the underlying concepts transfer to other systems like AMD ROCm and Intel oneAPI\n",
        "\n",
        "Let's import `CUDA.jl`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c71e750",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c71e750",
        "outputId": "6c76f95c-dd1e-4a96-a0b6-5ad7f9c2692e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n"
          ]
        }
      ],
      "source": [
        "using Pkg; Pkg.add(\"CUDA\"); Pkg.add(\"StaticArrays\")\n",
        "using CUDA, StaticArrays"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb681d6",
      "metadata": {
        "id": "3cb681d6"
      },
      "source": [
        "CUDA is actually made up of several components:\n",
        "- The GPU driver, which controls the graphics card and its interactions with the operating system kernel\n",
        "- The CUDA runtime, which provides the basic CUDA C extensions, CUDA API, and CUDA compiler\n",
        "- The CUDA libraries, which provide a \"GPUified\" implementation of BLAS (CUBLAS), RNG (CURAND), and many other things\n",
        "\n",
        "`CUDA.jl` has prepackaged [artifacts](https://pkgdocs.julialang.org/v1/artifacts/) for many CUDA versions, Julia versions, and OSes. It will download the appropriate one by default and install it for you. This lets us use the high-level Julia functions for CUDA arrays without worrying about the backend API calls (for now). Once `CUDA.jl` has installed, we can try creating some arrays on the GPU. We do this with `CuArray`, which turns a CPU array into a CUDA array by copying it.\n",
        "\n",
        "`CUDA.jl` also has some convenience constructors for `CuArray`s, like `CUDA.zeros`, `CUDA.rand`, and `undef` initializers. We can even use methods like `map` and `reduce` on these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93fe2a53",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93fe2a53",
        "outputId": "841a222b-5fc6-442c-f35e-310cdc914627"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CuArray{Float64, 2, CUDA.DeviceMemory}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "cu_a = CUDA.rand(Float64, 1024, 1024);\n",
        "typeof(cu_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4f2f0d4",
      "metadata": {
        "id": "c4f2f0d4"
      },
      "outputs": [],
      "source": [
        "cu_b = map(x->x^2, cu_a);"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3168a00c",
      "metadata": {
        "id": "3168a00c"
      },
      "source": [
        "These `CuArray`s live in **device memory** (GPU RAM), which means their individual elements are only available **from code executing on the GPU**. We know that the GPU memory is relatively limited compared to CPU memory, so how does `CUDA.jl` handle this? Do we have to explicitly \"reap\" these arrays ourselves?\n",
        "\n",
        "`CUDA.jl` extends the Julia garbage collector to work with GPU arrays as well. It manages a memory pool of GPU memory and garbage-collected `CuArray`s are in fact returned to the memory pool (rather than being entirely deallocated) in order to reduce GC pressure. `CUDA.jl` recycles arrays in the pool preferentially to speed up future allocations and keep an eye on how close we are to running out of memory. We can inspect the current state of the memory pool with `CUDA.pool_status()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5065b29f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5065b29f",
        "outputId": "2298834d-71b9-4b7b-bc1d-1d3bd5d2170b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective GPU memory usage: 6.74% (1016.938 MiB/14.741 GiB)\n",
            "Memory pool usage: 33.519 MiB (832.000 MiB reserved)\n"
          ]
        }
      ],
      "source": [
        "CUDA.pool_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d9f4311",
      "metadata": {
        "id": "2d9f4311"
      },
      "source": [
        "Let's allocate another device array and see how the pool status changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3504cadd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3504cadd",
        "outputId": "47167704-b76e-408f-dc38-6cb2066e37ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective GPU memory usage: 6.74% (1016.938 MiB/14.741 GiB)\n",
            "Memory pool usage: 49.519 MiB (832.000 MiB reserved)\n"
          ]
        }
      ],
      "source": [
        "cu_c = CUDA.rand(Float64, 1024, 1024)\n",
        "cu_c .+= cu_c' # make cu_c symmetric\n",
        "CUDA.pool_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9945587",
      "metadata": {
        "id": "e9945587"
      },
      "source": [
        "We should never need to explicitly deallocate CUDA arrays, but we can reduce GC pressure and help CUDA.jl out by indicating when we are done with an array, using `CUDA.unsafe_free!`. As you can guess from the name, you should **never** refer to the array after you've unsafely freed it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4065bcbb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4065bcbb",
        "outputId": "d03076a9-1de4-4c58-8e0c-2c4c4952a73a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective GPU memory usage: 6.74% (1016.938 MiB/14.741 GiB)\n",
            "Memory pool usage: 41.519 MiB (832.000 MiB reserved)\n"
          ]
        }
      ],
      "source": [
        "CUDA.unsafe_free!(cu_b)\n",
        "CUDA.pool_status()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a3c5d7",
      "metadata": {
        "id": "39a3c5d7"
      },
      "source": [
        "### Tips and tricks for higher level array operations\n",
        "\n",
        "In general, a `map` or `broadcast` over columns/slices launches multiple independent kernels which is usually slower than launching one larger kernel. For example,\n",
        "```julia\n",
        "broadcast(eachcol(a)) do x\n",
        "    prod(x)\n",
        "end\n",
        "```\n",
        "will usually be slower than `prod(a; dims=2)`.\n",
        "\n",
        "`CUDA.jl` isn't a tensor compiler and generally implements scalar (single-element) kernels for many operations like `map`. It's up to you to effectively use tools like broadcasting to avoid this.\n",
        "\n",
        "Broadcast fusion is especially helpful for GPU arrays because it turns multiple kernel launches on the same data into one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a53ff1c",
      "metadata": {
        "id": "2a53ff1c"
      },
      "source": [
        "## Using `LinearAlgebra` extensions\n",
        "\n",
        "Already we can see many of the nice array features we covered yesterday apply just as well to `CuArray`s. Many, but not all. `view`s, for example, are inconsistently supported. We can also make use of many wrapped linear algebra functions optimized for GPU via `CUBLAS`. We'll set the `CUBLAS` logger to prove to ourselves that it is in fact being used from our nice compact call:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1658577b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1658577b",
        "outputId": "3c9dd4de-3aca-4124-a732-3b2e6e81e8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I! cuBLAS (v12.3) function cublasStatus_t cublasGemmEx(cublasHandle_t, cublasOperation_t, cublasOperation_t, int, int, int, const void*, const void*, cudaDataType, int, const void*, cudaDataType, int, const void*, void*, cudaDataType, int, cublasComputeType_t, cublasGemmAlgo_t) called:\n",
            "i!  handle: type=cublasHandle_t; val=POINTER (IN HEX:0x0x13145540)\n",
            "i!  transa: type=cublasOperation_t; val=CUBLAS_OP_N(0)\n",
            "i!  transb: type=cublasOperation_t; val=CUBLAS_OP_N(0)\n",
            "i!  m: type=int; val=1024\n",
            "i!  n: type=int; val=1024\n",
            "i!  k: type=int; val=1024\n",
            "i!  alpha: type=void; val=POINTER (IN HEX:0x0x303800400)\n",
            "i!  A: type=void; val=POINTER (IN HEX:0x0x303000000)\n",
            "i!  Atype: type=cudaDataType_t; val=CUDA_R_64F(1)\n",
            "i!  lda: type=int; val=1024\n",
            "i!  B: type=void; val=POINTER (IN HEX:0x0x302000000)\n",
            "i!  Btype: type=cudaDataType_t; val=CUDA_R_64F(1)\n",
            "i!  ldb: type=int; val=1024\n",
            "i!  beta: type=void; val=POINTER (IN HEX:0x0x303800800)\n",
            "i!  C: type=void; val=POINTER (IN HEX:0x0x302800000)\n",
            "i!  Ctype: type=cudaDataType_t; val=CUDA_R_64F(1)\n",
            "i!  ldc: type=int; val=1024\n",
            "i!  computeType: type=cublasComputeType_t; val=CUBLAS_COMPUTE_64F(70)\n",
            "i!  algo: type=SOME TYPE; val=CUBLAS_GEMM_DEFAULT(-1)\n",
            "i! Time: 2025-07-14T23:55:46 elapsed from start 2.533333 minutes or 152.000000 seconds\n",
            "i!Process=701; Thread=135130737855744; GPU=0; Handle=POINTER (IN HEX:0x0x13145540); StreamId=POINTER (IN HEX:0x0xde924c0); MathMode=CUBLAS_DEFAULT_MATH | CUBLAS_MATH_DISALLOW_REDUCED_PRECISION_REDUCTION\n",
            "i! COMPILED WITH: GNU GCC/G++ / 6.3.1 20170216 (Red Hat 6.3.1-3)\n"
          ]
        }
      ],
      "source": [
        "using LinearAlgebra\n",
        "CUBLAS.cublasLoggerConfigure(1, 0, 1, C_NULL) # normally we don't do this, just for demonstration purposes here\n",
        "cu_d = cu_c * cu_a;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1077083",
      "metadata": {
        "id": "a1077083"
      },
      "source": [
        "We also have wrappers for some factorizations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50faff7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50faff7e",
        "outputId": "69db594e-969c-472f-c53a-87b372c4c1b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "I! cuBLAS (v12.3) function cublasStatus_t cublasDgemm_v2_64(cublasHandle_t, cublasOperation_t, cublasOperation_t, int64_t, int64_t, int64_t, const double*, const double*, int64_t, const double*, int64_t, const double*, double*, int64_t) called:\n",
            "i!  handle: type=cublasHandle_t; val=POINTER (IN HEX:0x0x136315a0)\n",
            "i!  transa: type=cublasOperation_t; val=CUBLAS_OP_C(2)\n",
            "i!  transb: type=cublasOperation_t; val=CUBLAS_OP_N(0)\n",
            "i!  m: type=long; val=16\n",
            "i!  n: type=long; val=16\n",
            "i!  k: type=long; val=64\n",
            "i!  alpha: type=double; val=POINTER (IN HEX:0x0x7ae65b7f96a0)\n",
            "i!  A: type=double; val=POINTER (IN HEX:0x0x30398c400)\n",
            "i!  lda: type=long; val=64\n",
            "i!  B: type=double; val=POINTER (IN HEX:0x0x30398e400)\n",
            "i!  ldb: type=long; val=64\n",
            "i!  beta: type=double; val=POINTER (IN HEX:0x0x7ae65b7f9698)\n",
            "i!  C: type=double; val=POINTER (IN HEX:0x0x303848400)\n",
            "i!  ldc: type=long; val=16\n",
            "i! Time: 2025-07-14T23:55:46 elapsed from start 2.533333 minutes or 152.000000 seconds\n",
            "i!Process=701; Thread=135130737855744; GPU=0; Handle=POINTER (IN HEX:0x0x136315a0); StreamId=POINTER (IN HEX:0x0xde924c0); MathMode=CUBLAS_DEFAULT_MATH\n",
            "i! COMPILED WITH: GNU GCC/G++ / 6.3.1 20170216 (Red Hat 6.3.1-3)\n",
            "I! cuBLAS (v12.3) function cublasStatus_t cublasDgemm_v2_64(cublasHandle_t, cublasOperation_t, cublasOperation_t, int64_t, int64_t, int64_t, const double*, const double*, int64_t, const double*, int64_t, const double*, double*, int64_t) called:\n",
            "i!  handle: type=cublasHandle_t; val=POINTER (IN HEX:0x0x136315a0)\n",
            "i!  transa: type=cublasOperation_t; val=CUBLAS_OP_C(2)\n",
            "i!  transb: type=cublasOperation_t; val=CUBLAS_OP_N(0)\n",
            "i!  m: type=long; val=16\n",
            "i!  n: type=long; val=16\n",
            "i!  k: type=long; val=16\n",
            "i!  alpha: type=double; val=POINTER (IN HEX:0x0x7ae65b7f96a0)\n",
            "i!  A: type=double; val=POINTER (IN HEX:0x0x303828400)\n",
            "i!  lda: type=long; val=128\n",
            "i!  B: type=double; val=POINTER (IN HEX:0x0x303848400)\n",
            "i!  ldb: type=long; val=16\n",
            "i!  beta: type=double; val=POINTER (IN HEX:0x0x7ae65b7f9698)\n",
            "i!  C: type=double; val=POINTER (IN HEX:0x0x303848c00)\n",
            "i!  ldc: type=long; val=16\n",
            "i! Time: 2025-07-14T23:55:46 elapsed from start 2.533333 minutes or 152.000000 seconds\n",
            "i!Process=701; Thread=135130737855744; GPU=0; Handle=POINTER (IN HEX:0x0x136315a0); StreamId=POINTER (IN HEX:0x0xde924c0); MathMode=CUBLAS_DEFAULT_MATH\n",
            "i! COMPILED WITH: GNU GCC/G++ / 6.3.1 20170216 (Red Hat 6.3.1-3)\n",
            "I! cuBLAS (v12.3) function cublasStatus_t cublasDgemm_v2_64(cublasHandle_t, cublasOperation_t, cublasOperation_t, int64_t, int64_t, int64_t, const double*, const double*, int64_t, const double*, int64_t, const double*, double*, int64_t) called:\n",
            "i!  handle: type=cublasHandle_t; val=POINTER (IN HEX:0x0x136315a0)\n",
            "i!  transa: type=cublasOperation_t; val=CUBLAS_OP_N(0)\n",
            "i!  transb: type=cublasOperation_t; val=CUBLAS_OP_N(0)\n",
            "i!  m: type=long; val=64\n",
            "i!  n: type=long; val=16\n",
            "i!  k: type=long; val=16\n",
            "i!  alpha: type=double; val=POINTER (IN HEX:0x0x7ae65b7f96a8)\n",
            "i!  A: type=double; val=POINTER (IN HEX:0x0x30398c400)\n",
            "i!  lda: type=long; val=64\n",
            "i!  B: type=double; val=POINTER (IN HEX:0x0x303848c00)\n",
            "i!  ldb: type=long; val=16\n",
            "i!  beta: type=double; val=POINTER (IN HEX:0x0x7ae65b7f96a0)\n",
            "i!  C: type=double; val=POINTER (IN HEX:0x0x30398e400)\n",
            "i!  ldc: type=long; val=64\n",
            "i! Time: 2025-07-14T23:55:46 elapsed from start 2.533333 minutes or 152.000000 seconds\n",
            "i!Process=701; Thread=135130737855744; GPU=0; Handle=POINTER (IN HEX:0x0x136315a0); StreamId=POINTER (IN HEX:0x0xde924c0); MathMode=CUBLAS_DEFAULT_MATH\n",
            "i! COMPILED WITH: GNU GCC/G++ / 6.3.1 20170216 (Red Hat 6.3.1-3)\n"
          ]
        }
      ],
      "source": [
        "qr_c = qr(CUDA.rand(Float64, 64, 32));"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f9b8e2",
      "metadata": {
        "id": "75f9b8e2"
      },
      "source": [
        "## Running multiple functions simultaneously\n",
        "\n",
        "A `1024 x 1024` matrix is decently hefty but not big enough to saturate the throughput of the GPU. If we have many such matrices, we can spawn multiple separate simultaneous executions on the same GPU to speed up our computation, provided we don't run out of memory. `CUDA.jl` has a nice integration with the built-in Julia threading for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a217c3b",
      "metadata": {
        "id": "5a217c3b"
      },
      "outputs": [],
      "source": [
        "n_arrs = 100\n",
        "cpu_arrays = [rand(Float64, 1024, 1024) for ix_arr in 1:n_arrs]\n",
        "results = Vector{Float64}(undef, n_arrs)\n",
        "@sync begin\n",
        "    for ix_arr in 1:n_arrs\n",
        "        Threads.@spawn begin\n",
        "            results[ix_arr] = mapreduce(sin, *, CuArray(cpu_arrays[ix_arr]))\n",
        "        end\n",
        "    end\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b591d673",
      "metadata": {
        "id": "b591d673"
      },
      "source": [
        "Obviously this example is a bit contrived, but using this basic pattern allows Julia and CUDA to work together to interleave memory copies and useful work, making better overall use of your GPU. We wrap everything in the `@sync` block because `Threads.@spawn` returns immediately after creating and launching its task, and `@sync` forces Julia to wait for all the `@spawn`-ed tasks to complete (and thus for all elements of `results` to be populated)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "248de6e2",
      "metadata": {
        "id": "248de6e2"
      },
      "source": [
        "## Important considerations for memory transfers\n",
        "\n",
        "- Memory copies to/from/across the GPU are **high latency**. Each one takes a relatively long time to kick off. Thus, it's usually better to do **one large copy** (by concatenating arrays, for example) than **many small copies**.\n",
        "- By default, `CuArray`s live in the GPU memory and can't be accessed elementwise from the CPU. You, as the programmer, explicitly request memory copies, which helps you control and understand when they happen. However, you can use the CUDA [Unified Memory](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/) [from `CUDA.jl`](https://cuda.juliagpu.org/dev/usage/memory/#Unified-memory), which makes the arrays visible from both CPU and GPU. The CUDA driver copies memory back and forth for you as needed. This can be very convenient for the programmer but can lead to degraded performance depending on your memory access pattern.\n",
        "- If you have a pre-allocated CPU array you'll be copying to and from frequently (a pre-allocated out of loop buffer, for example), you can **pin** the CPU memory using `CUDA.pin`, which can speed up these copies substantially. The very act of pinning is itself time-consuming, so be sure it's worth it (benchmarks!)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11fabbef",
      "metadata": {
        "id": "11fabbef"
      },
      "source": [
        "## When the builtin wrappers aren't enough: writing our own kernels ðŸŒ½\n",
        "\n",
        "Sometimes the high level Julia constructs just aren't enough. We need to implement our own high-performance functions that will be executed across the massive thread population. Awesome! However, there are some new complications:\n",
        "- We have to make efficient use of the hardware, which isn't always intuitive\n",
        "- We have a restricted subset of the programming language available on-GPU\n",
        "- For many operations it's not obvious how to translate the algorithm to an efficient GPU implementation\n",
        "\n",
        "A good example of this latter point is creating an [alias table](https://github.com/LilithHafner/AliasTables.jl/) for discrete categorical sampling (this is a common thing to do for sampling state vectors, for example). Many CPU algorithms exist for this purpose, and some GPU ones too, but the underlying approach is quite different because of the GPU's constraints.\n",
        "\n",
        "A function we will execute in a massively/\"embarrassingly\" parallel way on the GPU across a large array is called a *kernel*. We can write a restricted subset of Julia within a kernel. `CUDA.jl` and its backend `GPUCompiler.jl` handle translating & compiling this through LLVM to the underlying GPU IR.\n",
        "\n",
        "The higher-level functions we worked with before are actually implemented behind-the-scenes as CUDA kernels. It's worth spending a little time to understand what is happening when a kernel executes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754f9a25",
      "metadata": {
        "id": "754f9a25"
      },
      "source": [
        "### Quick guide to kernel launches and execution\n",
        "\n",
        "CUDA spawns a *grid* of *blocks* of *threads*, and each thread is an individual worker. Threads are grouped into *warps* of size 32. Each thread in the warp steps through the function instructions **together**. If two threads in the same warp encounter **different** instructions, the warp can no longer execute in parallel and the performance benefit of using the GPU can disappear. This is called **warp divergence** (very *Star Trek*). In general, one warp out of 10,000 experiencing divergence will not hurt things too badly -- but a large fraction of warps experiencing it will.\n",
        "\n",
        "**Our goal when writing CUDA kernels is to make efficient use of warp-parallelism.**\n",
        "\n",
        "When we launch the CUDA kernel, we launch it with `(n_blocks_x, n_blocks_y, n_blocks_z)` blocks of `(n_threads_x, n_threads_y, n_threads_z)` threads. Any of these numbers can be 1 -- but you can also create 3D grids if it's more natural for your problem. In general, it's **not** the case that `n_blocks_x * n_blocks_y * n_blocks_z * n_threads_x * n_threads_y * n_threads_z` threads execute in parallel (though this can happen if the sizes are small). Rather, blocks are scheduled by the CUDA driver, so that block 0 may execute, and then block 65, then block 32. **Block execution order is not guaranteed!**\n",
        "\n",
        "Because the warps are size 32, it is most efficient to make the thread dimensions a power of 2. So, `n_threads_x = 64` would be a good choice, or `n_threads_y = 512`. **In general**, larger thread counts are better (512 threads-per-block is usually more performant than 64) but of course there are some edge cases.\n",
        "\n",
        "Within a kernel, each thread has access to some information about its personal location in the overall grid. We can query the `threadIdx()` and `blockIdx()` functions to get the thread's `x`, `y`, and `z` thread and block positions, and `blockDim()` to get the number of threads in a block in the `x`, `y`, and `z` directions.\n",
        "\n",
        "We start by writing a `function` which may take arguments. `CUDA.jl` kernels aren't yet hooked up to standard Julia IO, but we **can** do some basic printing using the `@cuprintln` macro. **CUDA kernels must always return `nothing`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e322b77d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e322b77d",
        "outputId": "6fb0d569-0f01-491c-bcd0-0410621bacee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "my_first_cuda_kernel (generic function with 3 methods)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "function my_first_cuda_kernel(a)\n",
        "    t_x = threadIdx().x\n",
        "    b_x = blockIdx().x\n",
        "    l_a = length(a)\n",
        "    @cuprintln(\"Hi! My thread index is $t_x and my block index is $b_x. The length of my argument is $l_a.\")\n",
        "    return\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77f7a047",
      "metadata": {
        "id": "77f7a047"
      },
      "source": [
        "Now we *launch* the kernel on the GPU using the `@cuda` macro. We can specify how many threads and blocks to launch with at this time. **Keep in mind that CUDA kernels cannot see arrays in CPU memory, only GPU arrays.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7d53524",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7d53524",
        "outputId": "c4154859-7ae5-4993-ef41-21fac6766a24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUDA.HostKernel for my_first_cuda_kernel(CuDeviceVector{Float32, 1})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "cu_a = CUDA.rand(Float32, 512)\n",
        "\n",
        "@cuda threads=64 blocks=div(length(cu_a), 64) my_first_cuda_kernel(cu_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7aa79ca",
      "metadata": {
        "id": "e7aa79ca"
      },
      "source": [
        "However, **stack-allocated objects** like scalar integers, floats, or *tuples* (and thus *StaticArrays*) **can** be passed as arguments to CUDA kernels. ðŸ˜ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67eee3ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67eee3ec",
        "outputId": "806e6d31-4b4c-425e-ec5c-378c8a014f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My thread index is 1 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 4. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 8. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 2. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 3. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 7. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 1 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 17 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 18 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 19 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 20 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 21 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 22 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 23 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 24 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 25 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 26 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 27 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 28 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 29 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 30 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 31 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 32 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 6. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 33 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 34 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 35 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 36 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 37 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 38 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 39 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 40 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 41 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 42 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 43 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 44 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 45 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 46 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 47 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 48 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 49 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 50 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 51 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 52 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 53 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 54 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 55 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 56 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 57 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 58 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 59 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 60 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 61 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 62 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 63 and my block index is 5. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 64 and my block index is 5. Wowow Float32 so speedy.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUDA.HostKernel for my_first_cuda_kernel(SVector{16, ComplexF64})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My thread index is 1 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 2 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 3 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 4 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 5 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 6 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 7 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 8 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 9 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 10 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 11 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 12 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 13 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 14 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 15 and my block index is 1. The length of my argument is 16.\n",
            "Hi! My thread index is 16 and my block index is 1. The length of my argument is 16.\n"
          ]
        }
      ],
      "source": [
        "using StaticArrays\n",
        "\n",
        "static_a = SVector{16, ComplexF64}(rand(ComplexF64) for si in 1:16)\n",
        "@cuda threads=16 my_first_cuda_kernel(static_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e1473b",
      "metadata": {
        "id": "d4e1473b"
      },
      "source": [
        "### Specifying type constraints in CUDA kernel arguments\n",
        "\n",
        "From the point of view of a CUDA kernel, a `CuArray{T}` is a `CuDeviceArray{T}`. This is a new type that implements GPU-compatible operations, and the conversion is handled automatically for you. You can use multiple dispatch to define different kernels for different element types or array dimension and call the appropriate kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e5a78f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5a78f3",
        "outputId": "d04f391e-226c-47f5-8d1f-2f8e53c44e26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "my_first_cuda_kernel (generic function with 3 methods)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "function my_first_cuda_kernel(a::CuDeviceArray{Float32})\n",
        "    t_x = threadIdx().x\n",
        "    b_x = blockIdx().x\n",
        "    @cuprintln(\"Hi! My thread index is $t_x and my block index is $b_x. Wowow Float32 so speedy.\")\n",
        "    return\n",
        "end\n",
        "\n",
        "function my_first_cuda_kernel(a::CuDeviceArray{Float16})\n",
        "    t_x = threadIdx().x\n",
        "    b_x = blockIdx().x\n",
        "    @cuprintln(\"Hi! My thread index is $t_x and my block index is $b_x. Float16 so slender.\")\n",
        "    return\n",
        "end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2812a30e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2812a30e",
        "outputId": "107a0020-c7f9-4ebb-e333-761853147d62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My thread index is 1 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 2 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 3 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 4 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 5 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 6 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 7 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 8 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 9 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 10 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 11 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 12 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 13 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 14 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 15 and my block index is 1. Wowow Float32 so speedy.\n",
            "Hi! My thread index is 16 and my block index is 1. Wowow Float32 so speedy.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUDA.HostKernel for my_first_cuda_kernel(CuDeviceVector{Float32, 1})"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "CUDA.@sync @cuda threads=16 my_first_cuda_kernel(CUDA.rand(Float32, 16))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "833c3570",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "833c3570",
        "outputId": "00392678-fe34-46c2-dc00-d2c0d53e8bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi! My thread index is 1 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 2 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 3 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 4 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 5 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 6 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 7 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 8 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 9 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 10 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 11 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 12 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 13 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 14 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 15 and my block index is 1. Float16 so slender.\n",
            "Hi! My thread index is 16 and my block index is 1. Float16 so slender.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUDA.HostKernel for my_first_cuda_kernel(CuDeviceVector{Float16, 1})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "CUDA.@sync @cuda threads=16 my_first_cuda_kernel(CUDA.rand(Float16, 16))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e650d08a",
      "metadata": {
        "id": "e650d08a"
      },
      "source": [
        "## Avoiding warp divergence\n",
        "\n",
        "As mentioned above, threads in the same warp executing different instructions forces each thread to execute in serial, dramatically slowing execution. So does this mean we can never use `if` or `else`? Let's do some experiments to find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45d05dae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45d05dae",
        "outputId": "ac0d5e1d-fe9b-4710-f249-7c6bdc1dd7df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "diverging_kernel (generic function with 1 method)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "function diverging_kernel(arr)\n",
        "    my_flag = arr[threadIdx().x]\n",
        "    a = 1.0\n",
        "    n_iter = my_flag > 0 ? (my_flag + 1)*5 : 20\n",
        "    for ix in 1:n_iter\n",
        "        if my_flag > 0\n",
        "            a *= -2 * sin(rand() * Ï€/4)\n",
        "        else\n",
        "            a += sqrt(0.25)\n",
        "        end\n",
        "    end\n",
        "    return\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b654f32",
      "metadata": {
        "id": "6b654f32"
      },
      "source": [
        "Let's try launching and timing this with various values for `arr`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "293787a3",
      "metadata": {
        "id": "293787a3"
      },
      "outputs": [],
      "source": [
        "@cuda threads=64 diverging_kernel(CUDA.zeros(Int, 64));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "615b96d8-9923-4fc0-b365-6c3c57856312",
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "615b96d8-9923-4fc0-b365-6c3c57856312",
        "outputId": "cff2bcc8-10d8-41b0-c389-b1b101ddd2a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Profiler ran for 4.86 ms, capturing 12 events.\n",
              "\n",
              "Host-side activity: calling CUDA APIs took 31.47 Âµs (0.65% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name           â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚    0.63% â”‚\u001b[31m   30.52 Âµs â”‚     1 â”‚\u001b[1m cuLaunchKernel â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "\n",
              "Device-side activity: GPU was busy for 4.76 ms (97.84% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name                                         â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚   97.84% â”‚\u001b[31m    4.76 ms â”‚     1 â”‚\u001b[1m diverging_kernel(CuDeviceArray<Int64, 1, 1>) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "cu_arr = CuArray(rand(0:1, 64))\n",
        "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "717b330e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "717b330e",
        "outputId": "ca5cef69-dbc5-4cda-ce90-2460add6a0ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Profiler ran for 2.49 ms, capturing 12 events.\n",
              "\n",
              "Host-side activity: calling CUDA APIs took 31.71 Âµs (1.27% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name           â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚    1.20% â”‚\u001b[31m    29.8 Âµs â”‚     1 â”‚\u001b[1m cuLaunchKernel â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "\n",
              "Device-side activity: GPU was busy for 2.39 ms (95.93% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name                                         â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚   95.93% â”‚\u001b[31m    2.39 ms â”‚     1 â”‚\u001b[1m diverging_kernel(CuDeviceArray<Int64, 1, 1>) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "cu_arr = CuArray(vcat(zeros(Int, 32), ones(Int, 32)))\n",
        "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb220e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bb220e4",
        "outputId": "8fe2893e-da54-45a5-89d8-44733e8dcf6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Profiler ran for 4.85 ms, capturing 12 events.\n",
              "\n",
              "Host-side activity: calling CUDA APIs took 31.23 Âµs (0.64% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name           â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚    0.61% â”‚\u001b[31m   29.33 Âµs â”‚     1 â”‚\u001b[1m cuLaunchKernel â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "\n",
              "Device-side activity: GPU was busy for 4.75 ms (98.02% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name                                         â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚   98.02% â”‚\u001b[31m    4.75 ms â”‚     1 â”‚\u001b[1m diverging_kernel(CuDeviceArray<Int64, 1, 1>) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "cu_arr = CUDA.ones(Int, 64)\n",
        "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc3bd84",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efc3bd84",
        "outputId": "3d70d91f-57e7-4bc7-d72b-d1d0d35ad001"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Profiler ran for 9.6 ms, capturing 12 events.\n",
              "\n",
              "Host-side activity: calling CUDA APIs took 30.28 Âµs (0.32% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name           â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚    0.30% â”‚\u001b[31m   28.61 Âµs â”‚     1 â”‚\u001b[1m cuLaunchKernel â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "\n",
              "Device-side activity: GPU was busy for 9.5 ms (98.99% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Name                                         â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚   98.99% â”‚\u001b[31m     9.5 ms â”‚     1 â”‚\u001b[1m diverging_kernel(CuDeviceArray<Int64, 1, 1>) â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "cu_arr = CuArray(rand(-1:3, 64))\n",
        "CUDA.@profile @cuda threads=64 blocks=50_000 diverging_kernel(cu_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21bbb1f",
      "metadata": {
        "id": "e21bbb1f"
      },
      "source": [
        "### Forbidden operations\n",
        "\n",
        "Not all valid Julia code is valid inside a CUDA kernel. In particular, within a kernel we **cannot use**:\n",
        "- `String`s, except inside a `@cuprint`/`@cuprintln`/`@cuprintf` macro call\n",
        "- `Exception`s\n",
        "- Allocations (with two exceptions)\n",
        "\n",
        "Additionally, type-unstable or uninferrable code may not be compilable and may generate long and sometimes cryptic errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81cd2515",
      "metadata": {
        "id": "81cd2515"
      },
      "source": [
        "## Working with memory inside kernels\n",
        "\n",
        "When a warp executes, it has access to 3 types of memory:\n",
        "- Global memory, or the on-device GPU RAM. This is the largest and slowest memory.\n",
        "- Local memory, which holds locally defined variables for each thread.\n",
        "- Shared memory, which is shared (wow, really?) among all the threads in the warp. This memory allows threads **to communicate with one another**.\n",
        "\n",
        "Shared memory is physically located next to the cores executing the code and is fastest. To allocate shared memory, we can either define a `CuStaticSharedArray(T, N)`, with `N` elements of type `T` (`N` must be known at compile time) or a `CuDynamicSharedArray(T, N)`, where `N` isn't known at compile-time but will be passed to the kernel at *launch* time. **Note** that when you're using shared memory, you should call `sync_threads()` after any updates to it to make sure all threads in the warp can \"see\" the update (avoiding a data race)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89ceaf2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89ceaf2c",
        "outputId": "4c7d7fb7-fe85-4920-9f14-b39176f0e96c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUDA.HostKernel for reverse_kernel(CuDeviceVector{Int64, 1})"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "function reverse_kernel(a::CuDeviceArray{T}) where T\n",
        "    i = threadIdx().x\n",
        "    b = CuDynamicSharedArray(T, length(a))\n",
        "    b[length(a)-i+1] = a[i]\n",
        "    sync_threads()\n",
        "    a[i] = b[i]\n",
        "    return\n",
        "end\n",
        "\n",
        "cu_a = CuArray([1, 2, 3, 4, 5, 6, 7, 8])\n",
        "\n",
        "@cuda threads=length(cu_a) shmem=sizeof(cu_a) reverse_kernel(cu_a)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fd92ba1",
      "metadata": {
        "id": "6fd92ba1"
      },
      "source": [
        "The `shmem` argument should be the *total length* of shared memory needed across *all threads*. Many advanced GPU algorithms use shared memory for their effectiveness, so it's worth being aware of. The above example was pretty small -- do you think this would work if `a` were very large? Would we have to modify our kernel?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "806498ea",
      "metadata": {
        "id": "806498ea"
      },
      "source": [
        "### Memory access patterns\n",
        "\n",
        "As we discussed yesterday, the order in which we access array elements can have big performance impacts. In general, accessing adjacent memory locations in GPU memory is **much** faster than accessing locations that are widely separated. This is because to read a single location, the thread(s) have to load a whole \"word\" of memory at a time, so two memory locations that are part of the same word can be read cheaply once the word is loaded. This is also sometimes called \"coalesced\" memory access. It's most efficient to align warp-reads with 32-byte boundary memory regions -- something that using *strides* can often help with! How much of an impact can this have? For a matrix-matrix multiplication, NVIDIA ran tests on a V100 GPU. The observed memory bandwith was:\n",
        "\n",
        "- Naive implementation: `12.8 GB/s`\n",
        "- With shared memory to coalesce reads from global memory: `140.2 GB/s`\n",
        "- Further optimizations from reduce bank conflicts: `199.4 GB/s`\n",
        "\n",
        "Data taken from the [CUDA C++ programming guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory). So, if your application involves a lot of reading and writing to memory, it's really worth figuring out how to use shared memory effectively. This is also something the profiler can help with."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b6c2413",
      "metadata": {
        "id": "0b6c2413"
      },
      "source": [
        "## Benchmarking and profiling\n",
        "\n",
        "`CUDA.jl` comes with a great integration with the NVIDIA CUDA profilers. This makes it easier to profile our kernels and find and fix performance problems. We can use the `CUDA.@profile` macro to see where we're spending our time. Let's revisit our threading example from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "539e356b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539e356b",
        "outputId": "3b78d8bb-f09c-4abf-ed45-c5e4ab485c43"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Profiler ran for 1.32 s, capturing 22681 events.\n",
              "\n",
              "Host-side activity: calling CUDA APIs took 188.59 ms (14.27% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Time distribution                     â”‚\u001b[1m Name                    â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚   17.03% â”‚\u001b[31m  224.97 ms â”‚   100 â”‚   2.25 ms Â± 0.92   (  1.83 â€¥ 8.82)    â”‚\u001b[1m cuMemcpyHtoDAsync       â”‚\n",
              "â”‚    2.62% â”‚\u001b[33m   34.66 ms â”‚   100 â”‚ 346.64 Âµs Â± 607.69 (  7.15 â€¥ 2028.94) â”‚\u001b[1m cuStreamCreate          â”‚\n",
              "â”‚    0.78% â”‚\u001b[33m   10.37 ms â”‚   300 â”‚  34.55 Âµs Â± 133.97 (  3.34 â€¥ 2244.0)  â”‚\u001b[1m cuMemAllocFromPoolAsync â”‚\n",
              "â”‚    0.48% â”‚    6.31 ms â”‚   100 â”‚  63.08 Âµs Â± 252.41 ( 14.54 â€¥ 1948.36) â”‚ cuMemcpyDtoHAsync       â”‚\n",
              "â”‚    0.27% â”‚    3.51 ms â”‚   200 â”‚  17.57 Âµs Â± 39.76  (  7.39 â€¥ 566.01)  â”‚ cuLaunchKernel          â”‚\n",
              "â”‚    0.09% â”‚    1.18 ms â”‚   200 â”‚   5.89 Âµs Â± 3.56   (  2.38 â€¥ 25.99)   â”‚ cuStreamSynchronize     â”‚\n",
              "â”‚    0.01% â”‚  141.38 Âµs â”‚    20 â”‚   7.07 Âµs Â± 8.63   (  3.34 â€¥ 37.67)   â”‚ cuMemFreeAsync          â”‚\n",
              "â”‚    0.00% â”‚   40.53 Âµs â”‚   100 â”‚ 405.31 ns Â± 452.75 (   0.0 â€¥ 3099.44) â”‚ cuDeviceGet             â”‚\n",
              "â”‚    0.00% â”‚   34.81 Âµs â”‚     1 â”‚                                       â”‚ cuStreamDestroy         â”‚\n",
              "â”‚    0.00% â”‚   20.74 Âµs â”‚   101 â”‚ 205.37 ns Â± 111.98 (   0.0 â€¥ 476.84)  â”‚ cuDeviceGetCount        â”‚\n",
              "â”‚    0.00% â”‚    1.43 Âµs â”‚     1 â”‚                                       â”‚ cuCtxSetCurrent         â”‚\n",
              "â”‚    0.00% â”‚  715.26 ns â”‚     1 â”‚                                       â”‚ cuCtxGetDevice          â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "\n",
              "Device-side activity: GPU was busy for 240.36 ms (18.19% of the trace)\n",
              "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
              "â”‚\u001b[1m Time (%) â”‚\u001b[1m Total time â”‚\u001b[1m Calls â”‚\u001b[1m Time distribution                    â”‚\u001b[1m Name                                                                                                                                                                                                                                                            â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚   14.48% â”‚\u001b[31m   191.3 ms â”‚   100 â”‚   1.91 ms Â± 0.33   (  1.67 â€¥ 3.21)   â”‚\u001b[1m [copy pageable to device memory]                                                                                                                                                                                                                                â”‚\n",
              "â”‚    3.64% â”‚   48.03 ms â”‚   100 â”‚ 480.31 Âµs Â± 19.4   (438.45 â€¥ 501.87) â”‚ partial_mapreduce_grid(sin, _, Float64, CartesianIndices<2, Tuple<OneTo<Int64>, OneTo<Int64>>>, CartesianIndices<2, Tuple<OneTo<Int64>, OneTo<Int64>>>, Val<true>, CuDeviceArray<Float64, 3, 1>, CuDeviceArray<Float64, 2, 1>)                                  â”‚\n",
              "â”‚    0.06% â”‚  818.49 Âµs â”‚   100 â”‚   8.18 Âµs Â± 0.35   (  7.39 â€¥ 9.06)   â”‚ partial_mapreduce_grid(identity, _, Float64, CartesianIndices<3, Tuple<OneTo<Int64>, OneTo<Int64>, OneTo<Int64>>>, CartesianIndices<3, Tuple<OneTo<Int64>, OneTo<Int64>, OneTo<Int64>>>, Val<true>, CuDeviceArray<Float64, 3, 1>, CuDeviceArray<Float64, 3, 1>) â”‚\n",
              "â”‚    0.02% â”‚  212.91 Âµs â”‚   100 â”‚   2.13 Âµs Â± 0.36   (  1.67 â€¥ 3.81)   â”‚ [copy device to pageable memory]                                                                                                                                                                                                                                â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "n_arrs = 100\n",
        "CUDA.@profile begin\n",
        "    cpu_arrays = [rand(Float64, 1024, 1024) for ix_arr in 1:n_arrs]\n",
        "    results = Vector{Float64}(undef, n_arrs)\n",
        "    @sync begin\n",
        "        for ix_arr in 1:n_arrs\n",
        "            Threads.@spawn begin\n",
        "                results[ix_arr] = mapreduce(sin, *, CuArray(cpu_arrays[ix_arr]))\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df4d5f4",
      "metadata": {
        "id": "2df4d5f4"
      },
      "source": [
        "We can also generate a trace to provide to NVIDIA's profile visualizer, which can give us lots of detailed information and tips about what to fix. We can either launch Julia [inside the `nsys` profiler](https://cuda.juliagpu.org/dev/development/profiling/#NVIDIA-Nsight-Systems) or generate the file by setting the `external=true` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1a4099",
      "metadata": {
        "id": "7a1a4099"
      },
      "outputs": [],
      "source": [
        "n_arrs = 100\n",
        "CUDA.@profile external=true begin\n",
        "    cpu_arrays = [rand(Float64, 1024, 1024) for ix_arr in 1:n_arrs]\n",
        "    results = Vector{Float64}(undef, n_arrs)\n",
        "    @sync begin\n",
        "        for ix_arr in 1:n_arrs\n",
        "            Threads.@spawn begin\n",
        "                results[ix_arr] = mapreduce(sin, *, CuArray(cpu_arrays[ix_arr]))\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "end"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb31844",
      "metadata": {
        "id": "2fb31844"
      },
      "source": [
        "You can then open the generated report file in NSight Systems and see a trace of what the GPU was doing when, how much load it was under, and use this information to improve your code (e.g. by overlapping kernel launches and memory copies). You can also use [NVIDIA NSight Compute](https://cuda.juliagpu.org/dev/development/profiling/#NVIDIA-Nsight-Compute) to do very detailed analysis on a single kernel and pinpoint things to fix.\n",
        "\n",
        "**In GPU programming, you absolutely need to benchmark your kernels and use the profiler to fix any issues.**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some advantages of using Julia for GPU programming\n",
        "\n",
        "- Easy to write kernels that are human-readable\n",
        "- Can embed assembly in kernels to squeeze out more performance if necessary\n",
        "- Good wrappers for most vendor libraries, including higher level APIs from `LinearAlgebra` and co.\n",
        "- Seamless integration of CUDA streams and `Task`s to make scheduling multiple kernels ergonomic\n",
        "- Good support for CUDA-aware MPI through `MPI.jl`\n",
        "- Surprisingly easy to achieve good performance with a small amount of Julia code\n",
        "- Relatively easy to implement stuff out of the sandbox that CUDA convenience libraries like `cuTile` provide\n",
        "- Julia's package extension system makes providing optional GPU acceleration much more user-friendly\n",
        "- A **lot** of Julia code \"just works\" on CUDA\n",
        "- Multiple dispatch to kernels is very helpful and cuts down on code bloat\n",
        "\n",
        "## Some issues remain\n",
        "- Concrete eval doesn't work particularly well -- Julia's compiler can often evaluate `sin(1)` at compile time, but `CUDA.jl` wants to call the underlying NVIDIA `sin` assembly operation at runtime\n",
        "- Not everything is wrapped (please open an issue!)\n",
        "- We do wrap many low level primitives (such as for warp synchronization) but getting within 1% of C++ performance can be hard"
      ],
      "metadata": {
        "id": "KPNR_9PDwTUl"
      },
      "id": "KPNR_9PDwTUl"
    },
    {
      "cell_type": "markdown",
      "id": "beff884f",
      "metadata": {
        "id": "beff884f"
      },
      "source": [
        "## More resources\n",
        "\n",
        "A lot of the tutorials and resources for CUDA and GPU programming in general are written in C or C++. Many of the concepts apply to writing performant kernels in Julia or any other language. Additionally, some older references don't account for new device features (like tensor cores) that NVIDIA has introduced over the years. The best technique is to implement something and profile it!\n",
        "\n",
        "- [CUDA C Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide)\n",
        "- [JuliaCon 2021 GPU workshop](https://github.com/maleadt/juliacon21-gpu_workshop) -- 3 hours worth of tutorial content\n",
        "- [`CUDA.jl` documentation](https://cuda.juliagpu.org/dev/)\n",
        "- [`KernelAbstractions.jl`](https://juliagpu.github.io/KernelAbstractions.jl/dev/) -- write once, run on any GPU\n",
        "- [`DaggerGPU.jl`](https://github.com/JuliaGPU/DaggerGPU.jl) -- graph-based scheduler that can take GPU resources into account\n",
        "- [Hands on with Julia for HPC on GPUs and CPUs](https://www.youtube.com/watch?v=RNmSqbG2MUc)\n",
        "- [Differentiable modeling on GPUs workshop](https://github.com/PTsolvers/gpu-workshop-JuliaCon23)\n",
        "- [`#gpu` channel on Julia slack](https://julialang.org/slack/)\n",
        "- [GPU category on Julia discourse](https://discourse.julialang.org/c/domain/gpu/11)\n",
        "- [JuliaGPU Zoom office hours](https://julialang.org/community/), scroll down for the calendar invite"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep dive into GEMM kernel optimization\n",
        "\n",
        "This pair of recent blog posts [(A)](https://jianyuh.github.io/gemm/optimization/hopper/2024/12/29/h100_gemm.html) and [(B)](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog) discusses optimizing the GEMM kernel for H100 CUDA devices. It includes a deep dive into **multiple** steps of optimization, which we can quickly summarize here:\n",
        "\n",
        "- Take advantage of the H100's tensor cores, which can compute matrix-matrix multiplication in a single instruction for small matrix sizes\n",
        "- Implement **tiling** of outputs, and use larger output tiles which can help with memory access\n",
        "- Hide *load* latency by interleaving tensor core ops and memory ops, which can be done with warp specialization\n",
        "- Use special warpgroups to work with the output tile (prevents register spilling)\n",
        "- Hide *store* latency by overlapping write operations for one thread block with load operations for another\n",
        "- Force nearby tiles to be scheduled at the same time to use the L2 cache\n",
        "- Use a device assembly instruction to implement a slightly faster barrier\n",
        "- Group multiple thread blocks together into clusters to allow the kernel to load the same tile to multiple SMs\n",
        "- A bunch of micro optimizations\n",
        "- Using asynchronous stores to move data from the SM registers back into global memory\n",
        "- Implement careful scheduling to improve L2 cache hit rate\n",
        "\n",
        "Even with all this work, the optimized kernel is only **7%** faster than `CUBLAS` in the best case, while `CUBLAS` is **generally** quite performant. Partly this is because NVIDIA (and other vendors) employs an army of very smart people to write performant kernels. There are two main lessons from this:\n",
        "\n",
        "1. Writing state of the art kernels is really hard\n",
        "2. You should try to use the vendor library unless you have a good reason not to"
      ],
      "metadata": {
        "id": "hIVdji_sbP4x"
      },
      "id": "hIVdji_sbP4x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using CUDA and other GPU libraries for QIS\n",
        "\n",
        "CUDA provides several libraries besides `CUBLAS` which are useful for us as quantum information scientists:\n",
        "\n",
        "- `CUTENSOR`, for tensor operations like permutation, contraction (Einstein summation), addition, etc.\n",
        "- `CUSTATEVEC`, a GPU-accelerated library for state vector simulation of quantum circuits\n",
        "- `CUTENSORNET`, a GPU-accelerated library for tensor network simulation of quantum circuits\n",
        "\n",
        "The latter two are quite focused on quantum circuit simulation. In addition, many higher level packages provide these as backend simulation options. For example, `ITensors.jl` and the `TensorKit.jl` ecosystems both offer `CUTENSOR` accelerated backends.\n",
        "\n",
        "Additionally, AMD offers their own `CUTENSOR` equivalent with an extremely similar API which is not yet wrapped by `AMDGPU.jl`.\n",
        "\n",
        "[`Yao.jl`](https://github.com/QuantumBFS/Yao.jl), a circuit simulation library, also has an extension for CUDA acceleration for simulations.\n",
        "\n",
        "In general, the higher level Julia libraries provide many convenience functions that the lower level CUDA libraries don't, and are portable to other HPC systems, so often the easier lift is to start with the high level package and file issues with `CUDA.jl` or `AMDGPU.jl` as needed to let maintainers (me & friends) fix things or add functionality."
      ],
      "metadata": {
        "id": "c3kdbYbbsRNc"
      },
      "id": "c3kdbYbbsRNc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More about `KernelAbstractions.jl` and `AcceleratedKernels.jl`\n",
        "\n",
        "As we said above, CUDA is probably the most mature GPU programming framework around, but AMD has put a lot of effort to improving their ROCm tooling for their GPUs, and Apple (Metal) and Intel (OneAPI) also offer GPU programming toolkits. Happily, Julia has a high level framework for targeting any of these backends, allowing you to write portable code that can run on various GPUs. This is particularly helpful if you have allocations at a variety of supercomputing centers, which tend to buy large sets of GPUs from either NVIDIA or AMD, and at which AMD is quite common.\n",
        "\n",
        "`AcceleratedKernels.jl` works with `KernelAbstractions.jl` to provide a truly portable platform for writing \"accelerated\" code, **but**, as the authors state, \"The API is starting to stabilise\". So, many features and libraries (like sparse support) you may expect from `CUDA.jl` or `AMDGPU.jl` may not be supported yet (PRs welcome!). We can take a look at some sample kernels, again intended to run on NVIDIA hardware since that's what Google CoLab provides."
      ],
      "metadata": {
        "id": "fkNG-ZahYDC5"
      },
      "id": "fkNG-ZahYDC5"
    },
    {
      "cell_type": "code",
      "source": [
        "Pkg.add(\"AcceleratedKernels\"); Pkg.add(\"KernelAbstractions\")\n",
        "using AcceleratedKernels, KernelAbstractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQCMm3uxY71h",
        "outputId": "46805e3b-8707-42f1-cae8-aca4e4ca7cbe"
      },
      "id": "bQCMm3uxY71h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Project.toml`\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n",
            "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
            "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/.julia/environments/v1.10/Project.toml`\n",
            "  \u001b[90m[63c18a36] \u001b[39m\u001b[92m+ KernelAbstractions v0.9.37\u001b[39m\n",
            "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.10/Manifest.toml`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = CUDA.rand(Float32, 10, 100_000); # LARGE"
      ],
      "metadata": {
        "id": "BXuQlYb8a644"
      },
      "id": "BXuQlYb8a644",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f(x) = x * x\n",
        "mrowsumsq = AcceleratedKernels.mapreduce(f, +, m; init=zero(eltype(m)), dims=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8EoRbRabP8X",
        "outputId": "73daf89a-236b-4c9d-d150-8f55558f646f"
      },
      "id": "m8EoRbRabP8X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1Ã—100000 CuArray{Float32, 2, CUDA.DeviceMemory}:\n",
              " 1.5963  4.19916  4.12753  2.98479  â€¦  3.0629  4.2008  3.8935  3.30144"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AcceleratedKernels.jl` provides many \"basic\" array operations like `map`, `mapreduce`, `sort` -- you can do a lot of powerful things with these! For writing your own kernels, you can also do some light piracy and work to understand some of how these kernels are implemented under the hood:"
      ],
      "metadata": {
        "id": "8JsWzz6dbjtY"
      },
      "id": "8JsWzz6dbjtY"
    },
    {
      "cell_type": "code",
      "source": [
        "@kernel inbounds=true cpu=false unsafe_indices=true function _mapreduce_block!(@Const(src), dst, f, op, neutral)\n",
        "\n",
        "    @uniform N = @groupsize()[1]\n",
        "    sdata = @localmem eltype(dst) (N,)\n",
        "\n",
        "    len = length(src)\n",
        "\n",
        "    # NOTE: for many index calculations in this library, computation using zero-indexing leads to\n",
        "    # fewer operations (also code is transpiled to CUDA / ROCm / oneAPI / Metal code which do zero\n",
        "    # indexing). Internal calculations will be done using zero indexing except when actually\n",
        "    # accessing memory. As with C, the lower bound is inclusive, the upper bound exclusive.\n",
        "\n",
        "    # Group (block) and local (thread) indices\n",
        "    iblock = @index(Group, Linear) - 0x1\n",
        "    ithread = @index(Local, Linear) - 0x1\n",
        "\n",
        "    i = ithread + iblock * (N * 0x2)\n",
        "    if i >= len\n",
        "        sdata[ithread + 0x1] = neutral\n",
        "    elseif i + N >= len\n",
        "        sdata[ithread + 0x1] = f(src[i + 0x1])\n",
        "    else\n",
        "        sdata[ithread + 0x1] = op(f(src[i + 0x1]), f(src[i + N + 0x1]))\n",
        "    end\n",
        "\n",
        "    @synchronize()\n",
        "\n",
        "    if N >= 512u16\n",
        "        if ithread < 256u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 256u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 256u16\n",
        "        if ithread < 128u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 128u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 128u16\n",
        "        if ithread < 64u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 64u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 64u16\n",
        "        if ithread < 32u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 32u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 32u16\n",
        "        if ithread < 16u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 16u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 16u16\n",
        "        if ithread < 8u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 8u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 8u16\n",
        "        if ithread < 4u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 4u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 4u16\n",
        "        if ithread < 2u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 2u16 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "    if N >= 2u16\n",
        "        if ithread < 1u16\n",
        "            sdata[ithread + 0x1] = op(sdata[ithread + 0x1], sdata[ithread + 0x1 + 0x1])\n",
        "        end\n",
        "        @synchronize()\n",
        "    end\n",
        "\n",
        "    # Code below would work on NVidia GPUs with warp size of 32, but create race conditions and\n",
        "    # return incorrect results on Intel Graphics. It would be useful to have a way to statically\n",
        "    # query the warp size at compile time\n",
        "    #\n",
        "    # if ithread < 32\n",
        "    #     N >= 64 && (sdata[ithread + 1] = op(sdata[ithread + 1], sdata[ithread + 32 + 1]))\n",
        "    #     N >= 32 && (sdata[ithread + 1] = op(sdata[ithread + 1], sdata[ithread + 16 + 1]))\n",
        "    #     N >= 16 && (sdata[ithread + 1] = op(sdata[ithread + 1], sdata[ithread + 8 + 1]))\n",
        "    #     N >= 8 && (sdata[ithread + 1] = op(sdata[ithread + 1], sdata[ithread + 4 + 1]))\n",
        "    #     N >= 4 && (sdata[ithread + 1] = op(sdata[ithread + 1], sdata[ithread + 2 + 1]))\n",
        "    #     N >= 2 && (sdata[ithread + 1] = op(sdata[ithread + 1], sdata[ithread + 1 + 1]))\n",
        "    # end\n",
        "\n",
        "    if ithread == 0x0\n",
        "        dst[iblock + 0x1] = sdata[0x1]\n",
        "    end\n",
        "end"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZCMJEw8cPCT",
        "outputId": "62f5a692-c629-4c29-f227-82a3c02c9c37"
      },
      "id": "6ZCMJEw8cPCT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_mapreduce_block! (generic function with 4 methods)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow! That is complicated and a big pain. However, this shows the amount of work that can go into writing a \"top line\" GPU kernel.\n",
        "\n",
        "### Tuning\n",
        "\n",
        "To get good performance out of a particular GPU for a particular use case, you may need to do (a **lot** of) \"tuning\". This involves benchmarking various usage scenarios -- more or less square matrix, \"hot dog shaped\" (technical term) matrix, triangular matrix, etc. -- on the real hardware and saving the results, which can then be used in a large decision tree to pick which kernel to launch. The tuning process can take many days or even weeks, and the results can be very large -- this is why some library shared object files are humongous (100s of MB). To perform tuning you will probably need a large amount of compute, so it is hard to do if you yourself aren't a GPU vendor or a supercomputing center.\n",
        "\n",
        "## One more interesting package: `GemmKernels.jl`\n",
        "\n",
        "A very common operation we'd like to perform on GPU is `GEMM` - general matrix multiplication. This operation is of extreme importance to ML/AI applications and GPU vendors and AI startups spend a lot of money to find people who can write good kernels for this. JuliaGPU has a package attemping to write some good open source GEMM kernels at `GemmKernels.jl` -- but the tuning process takes up to a week ðŸ¤¯."
      ],
      "metadata": {
        "id": "cJ_Nez5vcwE9"
      },
      "id": "cJ_Nez5vcwE9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Julia",
      "name": "julia"
    },
    "language_info": {
      "name": "julia"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}