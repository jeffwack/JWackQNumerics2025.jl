{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06bdbe6",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "-----\n",
    "## Goals\n",
    "- Gradient descent\n",
    "- Newton's method\n",
    "- Regularization\n",
    "- Newton approximations\n",
    "- Line search\n",
    "- KKT conditions\n",
    "- Exercises with GRAPE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332c8774",
   "metadata": {},
   "source": [
    "# Unconstrained Optimization\n",
    "\n",
    "$$ \\min_{\\bm x} J(\\bm x) $$\n",
    "\n",
    "### Necessary Conditions\n",
    "\n",
    "$$ \\nabla J ( \\bm{x}^* ) = \\bm{0} $$\n",
    "$$ \\nabla^2 J(\\bm{x}^*) \\succeq \\bm{0} $$\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "### Gradient Descent Update Rule\n",
    "\n",
    "The gradient descent algorithm updates the current point \\( x \\) by stepping in the direction opposite the gradient:\n",
    "\n",
    "$$ \\bm{x}_{i+1} = \\bm{x}_{i} - \\eta \\cdot \\nabla J(\\bm{x}_{i}) $$\n",
    "\n",
    "where $\\eta$ is the learning rate - controlling the step size.\n",
    "\n",
    "### Example solve\n",
    "\n",
    "We're going to apply **gradient descent** to minimize the following:\n",
    "\n",
    "$$ h(x) = x^4 + x^3 - x^2 - x $$\n",
    "\n",
    "This is a smooth, non-linear function with multiple stationary points. Our goal is to find a local minimum by starting from an initial value and following the gradient downhill: $\\nabla h(x^*) = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a858d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting stuffs\n",
    "using CairoMakie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function h(x)\n",
    "    return x.^4 + x.^3 - x.^2 - x\n",
    "end\n",
    "\n",
    "function ∇h(x)\n",
    "    return 4.0*x.^3 + 3.0*x.^2 - 2.0*x - 1.0\n",
    "end\n",
    "\n",
    "x = range(-1.75,1.25,1000)\n",
    "\n",
    "## Initial guess\n",
    "x₀ = 1.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d40865",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ = x₀\n",
    "# xᵢ = 0.0\n",
    "\n",
    "gradient_descent_step(xᵢ; η=0.01) = xᵢ - η * ∇h(xᵢ)\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d577a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = gradient_descent_step(xᵢ, η=0.01) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color=:orange, marker='x', markersize=25)\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b6d8c",
   "metadata": {},
   "source": [
    "## Newtons Method\n",
    "\n",
    "Now we're using a local quadratic approximation, for a nearby guess $x_k$ the value of $J(\\bm{x})$ can be well-approximated as\n",
    "\n",
    "$$ J(\\bm{x}) \\approx J(\\bm{x}_k) + \\nabla J(\\bm{x})^T (\\bm{x} - \\bm{x}_k) + \\frac{1}{2} (\\bm{x} - \\bm{x}_k)^T \\nabla^2 J(\\bm{x}_k) (\\bm{x} - \\bm{x}_k)$$\n",
    "\n",
    "And we can solve for our necessary condition\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\bm{0} & \\overset{\\mathrm{!}}{=} \\nabla J(\\bm{x} + \\Delta \\bm{x}) \\approx \\nabla J(\\bm{x}) + \\nabla^2 J(\\bm{x}) \\Delta \\bm{x} \\\\\n",
    "\\Delta \\bm{x} & \\overset{\\mathrm{!}}{=} - \\left(\\nabla^2 J(\\bm{x}) \\right)^{-1} \\nabla J(\\bm{x}) \\\\ \n",
    "\\end{align} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54027a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "function ∇²h(x)\n",
    "    return 12.0*x.^2 + 6.0*x - 2.0\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded3c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xᵢ = x₀\n",
    "xᵢ = 0.0\n",
    "newton_step(xᵢ) = xᵢ - ∇²h(xᵢ)\\∇h(xᵢ)\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e8c22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = newton_step(xᵢ) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color=:orange, marker='x', markersize=25)\n",
    "\n",
    "# plot the quadratic approximation of the function at xᵢ as a dotted line, \n",
    "# notice how the newton step moves to the argmin of the quadratic approximation\n",
    "\n",
    "# h_q(x, xᵢ) = h(xᵢ) .+ ∇h(xᵢ) * (x .- xᵢ) .+ 0.5 * ∇²h(xᵢ) * (x .- xᵢ).^2\n",
    "# y_min = minimum(h(x))\n",
    "# y_max = maximum(h(x))\n",
    "# ylims!(ax1, y_min - 0.5, y_max + 0.5)\n",
    "# lines!(ax1, x, h_q(x, xᵢ), color=:blue, linestyle=:dot, linewidth=2, alpha=0.2)\n",
    "\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fad87f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Quick check, what happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2cd07",
   "metadata": {},
   "source": [
    "## Add Regularization\n",
    "\n",
    "In the last example, our initialization violated our assumption of a positive semi-definite hessian\n",
    "\n",
    "$$ \\nabla^2 h(0) = -2 $$\n",
    "\n",
    "The first trick we can employ is regularization. The intuition is that we want to retain second order approximation of the function for better convergence, but the approximation must ensure that the direction of travel is still one of guarenteed descent.\n",
    "\n",
    "Check out some of [these visualizations](https://kenndanielso.github.io/mlrefined/blog_posts/7_Second_order_methods/7_4_Newtons_regularized.html) for some of the details and play around with the regularization parameter.\n",
    "\n",
    "As mentioned on that page, solving for the regularization parameter is impractical: guess and check approximations to this value can be sufficient and are easily computable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54296135",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "β = 1.0\n",
    "# β = 2.75\n",
    "\n",
    "function regularized_newton_step(xᵢ; β=β)\n",
    "    H = ∇²h(xᵢ)\n",
    "    while !isposdef(H) # don't do this in your own code!\n",
    "        H = H + β*I\n",
    "    end\n",
    "    return xᵢ - H\\∇h(xᵢ)\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = 1.19\n",
    "xᵢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = regularized_newton_step(xᵢ) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color=:orange, marker='x', markersize=25)\n",
    "\n",
    "# plot the quadratic approximation of the function at xᵢ as a dotted line with a regularized hessian,\n",
    "# function h_regularized_q(x, xᵢ)\n",
    "#     H = ∇²h(xᵢ)\n",
    "#     while !isposdef(H) # don't do this in your own code!\n",
    "#         H = H + β*I\n",
    "#     end\n",
    "#     println(\"H: \", H)\n",
    "#     return h(xᵢ) .+ ∇h(xᵢ) * (x .- xᵢ) .+ 0.5 * H * (x .- xᵢ).^2\n",
    "# end\n",
    "\n",
    "# ylims!(ax1, y_min - 0.5, y_max + 0.5)\n",
    "# xlims!(ax1, minimum(x) - 0.5, maximum(x) + 0.5)\n",
    "# lines!(ax1, x, h_regularized_q(x, xᵢ), color=:blue, linestyle=:dot, linewidth=2, alpha=0.2)\n",
    "\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2fb719",
   "metadata": {},
   "source": [
    "### How can this go wrong?\n",
    "\n",
    "For the regularization, we chose $\\beta = 1.0$ which worked well, but adding $\\beta * \\bm{I}$ to our hessian until it is positive semi-definite could be bad. If the hessian has large negative eigenvalues, it can take a *REALLY* long time - linear in the magnitude of the smallest eigenvalue.\n",
    "\n",
    "In practice, we use additional information to choose a good $\\beta$, maybe guessing and checking a few times, and then regularize the quadratic - ideally in just a few guesses and checks.\n",
    "\n",
    "If we change the regularization parameter to something too high, this will have convergence no better than gradient descent. If it was too low (but the hessian was still positive semi-definite) we also observed overshoot.\n",
    "\n",
    "**Final note:** regularization like this helps make the problem 'convex' in a way - another benefit to reap is that near flat areas, regularization can help ensure that the hessian does not become ill-conditioned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "function h(x)\n",
    "    return 0.125 * x.^4 + sin.(3 * x) .+ 0.5  * x\n",
    "end\n",
    "\n",
    "function ∇h(x)\n",
    "    return 0.5 * x.^3 + 3 * cos.(3 * x) .+ 0.5\n",
    "end\n",
    "\n",
    "function ∇²h(x)\n",
    "    return 1.5 * x.^2 - 9 * sin.(3 * x)\n",
    "end\n",
    "\n",
    "x = range(-3.75,3.25,1000)\n",
    "\n",
    "## Initial guess\n",
    "x₀ = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c94b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ = x₀\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871cf3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = regularized_newton_step(xᵢ; β=0.85)\n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color=:orange, marker='x', markersize=25)\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc9d7c",
   "metadata": {},
   "source": [
    "## Add line search\n",
    "\n",
    "Ok so that was bad, things went sideways and we were all over the place. This function was tailored to diverge with $\\beta = 0.85$. We want to make sure when we are taking a step, that the slope is not changing too quickly over the step. We want to take a step that sufficiently decreases the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da78e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "function backtracking_regularized_newton_step(xᵢ)\n",
    "    H = ∇²h(xᵢ)\n",
    "\n",
    "    ## regularization\n",
    "    β = 1.0\n",
    "    while !isposdef(H)\n",
    "        H = H + β*I\n",
    "    end\n",
    "    Δx = -H\\∇h(xᵢ)\n",
    "\n",
    "    ## line search\n",
    "    b = 0.1\n",
    "    c = 0.25\n",
    "    α = 1.0\n",
    "    while h(xᵢ + α*Δx) > h(xᵢ) + b*α*∇h(xᵢ)*Δx\n",
    "        α = c*α\n",
    "    end\n",
    "    \n",
    "    return xᵢ + α*Δx\n",
    "end\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = 1.19\n",
    "xᵢ = 0.0\n",
    "\n",
    "## Initial plot\n",
    "fig1 = Figure()\n",
    "ax1 = Axis(fig1[1,1])\n",
    "lines!(ax1, x, h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dda4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁ = backtracking_regularized_newton_step(xᵢ) \n",
    "plot!(ax1, [xᵢ], [h(xᵢ)], color=:green, marker='x', markersize=25)\n",
    "xᵢ = xᵢ₊₁\n",
    "fig1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd0d236",
   "metadata": {},
   "source": [
    "# II. Constrained Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694520a6",
   "metadata": {},
   "source": [
    "In this section, we are going to learn about *constrained optimization*. Let's keep things simple, and start without all of the regularization and line search stuff. \n",
    "\n",
    "In the next cells, we'll define a 2D bowl as our cost function, and we'll draw some nice level curves to visualize it--it's a convex cost, so we know it will have a minimum at the bottom of the bowl. To make it interesting, we will add a single constraint, which we draw as a curve.\n",
    "\n",
    "$$\n",
    "J(x, y) = \\frac{1}{2} \\left( \\tfrac{1}{2} (x - 1)^2 + (y - 1)^2 \\right)\n",
    "$$\n",
    "\n",
    "When constructing out system, to get to a minimum while respecting the contraints, we are using an augmented lagrangian. At each optimization step, we will enforce the contraints - more on this in a moment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aed371",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Diagonal([0.5; 1])\n",
    "\n",
    "## Objective\n",
    "function J(x)\n",
    "    return 1 / 2 * (x - [1; 0])' * Q * (x - [1; 0])\n",
    "end\n",
    "\n",
    "function ∇J(x)\n",
    "    return Q * (x - [1; 0])\n",
    "end\n",
    "\n",
    "function ∇²J(x)\n",
    "    return Q\n",
    "end\n",
    "\n",
    "## Linear constraint -- you can try this, also.\n",
    "# A = [1.0 -1.0]\n",
    "# b = -1.0\n",
    "# function f(x)\n",
    "#     return A * x - b\n",
    "# end\n",
    "\n",
    "# function ∂f(x)\n",
    "#     return A\n",
    "# end\n",
    "\n",
    "## Nonlinear constraint\n",
    "function f(x)\n",
    "    return x[1]^2 + 2*x[1] - x[2]\n",
    "end\n",
    "\n",
    "function ∂f(x)\n",
    "    return [2*x[1]+2 -1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020cf493",
   "metadata": {},
   "outputs": [],
   "source": [
    "function draw_contour(ax; samples=40, levels=25)\n",
    "    cols = kron(ones(samples), range(-4, 4, samples)')\n",
    "    rows = kron(ones(samples)', range(-4, 4, samples))\n",
    "    vals = zeros(samples,samples)\n",
    "    for j = 1:samples\n",
    "        for k = 1:samples\n",
    "            vals[j, k] = J([cols[j, k]; rows[j, k]])\n",
    "        end\n",
    "    end\n",
    "    contour!(ax, vec(cols), vec(rows), vec(vals), levels=levels)\n",
    "\n",
    "    ## Linear x - y + 1 = 0 -- uncomment this if you want to try linear constraint\n",
    "    # constraint = range(-4, 3, samples)\n",
    "    # lines!(ax, constraint, constraint .+ 1, color=:black, linewidth=2)\n",
    "\n",
    "    ## Nonlinear x^2 + 2x - y = 0\n",
    "    constraint = range(-3.2, 1.2, samples)\n",
    "    lines!(ax, constraint, constraint.^2 .+ 2*constraint, color=:black, linewidth=2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff\n",
    "\n",
    "function newton_step(xᵢ, λᵢ)\n",
    "    ∂²L_∂x² = ∇²J(xᵢ) + ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)\n",
    "    ∂f_∂x = ∂f(xᵢ)\n",
    "\n",
    "    ## KKT system\n",
    "    H = [∂²L_∂x² ∂f_∂x'; ∂f_∂x 0]\n",
    "    g = [∇J(xᵢ) + ∂f_∂x'λᵢ; f(xᵢ)]\n",
    "    \n",
    "    Δz = -H\\g\n",
    "    Δx = Δz[1:2]\n",
    "    Δλ = Δz[3]\n",
    "    return xᵢ .+ Δx, λᵢ .+ Δλ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1], aspect=1)\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = Float64[-0.75; -1.75]\n",
    "xᵢ = Float64[-3; 2]\n",
    "λᵢ = Float64[0.0]\n",
    "\n",
    "## Draw the initial contours and the initial guess\n",
    "draw_contour(ax)\n",
    "plot!(ax, [xᵢ[1]], [xᵢ[2]], color=:red, marker=:circle, markersize=15)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781ad942",
   "metadata": {},
   "outputs": [],
   "source": [
    "xᵢ₊₁, λᵢ₊₁ = newton_step(xᵢ, λᵢ)\n",
    "plot!(ax, [xᵢ₊₁[1]], [xᵢ₊₁[2]], color=:red, marker=:x, markersize=15)\n",
    "xᵢ .= xᵢ₊₁\n",
    "λᵢ .= λᵢ₊₁\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bf1a53",
   "metadata": {},
   "source": [
    "Let's talk about KKT systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b15946",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect the Hessian\n",
    "H = ∇²J(xᵢ) + ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727a514",
   "metadata": {},
   "source": [
    "We need regularization... even though we picked a convex cost! The constraint in our system makes this problem not convex anymore. Let's add regularization, but we will do so a bit differently.\n",
    "\n",
    "**ASIDE**\n",
    "Not in this talk, but that second term has other methods that can be used to approximate it (like LBFGS). LBDGS in particular has some robustness properites that baseline Newton's method lacks. We will make a similar approximation to our KKT system, called the *Gauss-Newton approximation*. See the exercises below for a more detailed explanation.\n",
    "\n",
    "The thought process is as follows: After inspecting `∂²L_∂x² = ∇²J(xᵢ) + ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)`, we note that $\\nabla^2 J$ is convex by construction. It is just the `ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)` that causes trouble with the Hessian. At this time, we also notice that latter term is also expensive to compute. Because it causes trouble and is costly to compute, we decide to drop this term. This is the Gauss-Newton approximation. Its steps compute faster, but converge slower than Newton--luckily, the savings in compute speed often overtake any reduction in convergence rate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330a3545",
   "metadata": {},
   "outputs": [],
   "source": [
    "function gauss_newton_step(xᵢ, λᵢ)\n",
    "    ## Implicit regularization\n",
    "    ∂²L_∂x² = ∇²J(xᵢ) #+ ForwardDiff.jacobian(x -> ∂f(x)'λᵢ, xᵢ)\n",
    "    ∂f_∂x = ∂f(xᵢ)\n",
    "\n",
    "    ## KKT system\n",
    "    H = [∂²L_∂x² ∂f_∂x'; ∂f_∂x 0]\n",
    "    g = [∇J(xᵢ) + ∂f_∂x'λᵢ; f(xᵢ)]\n",
    "    \n",
    "    Δz = -H\\g\n",
    "    Δx = Δz[1:2]\n",
    "    Δλ = Δz[3]\n",
    "    return xᵢ .+ Δx, λᵢ .+ Δλ\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046c8fb4",
   "metadata": {},
   "source": [
    "This quick calculation should hopefully remind you about the Gauss-Newton approximation.\n",
    "\n",
    "Start with a cost $J(\\mathbf{x})$. The necessary condition for optimality is $\\nabla J(\\mathbf{x}) = 0$. Our journey starts by asking what happens if $J(\\mathbf{x})$ is actually a least squares problem. For example, $J(\\mathbf{x}) := \\frac{1}{2}||\\mathbf{g}(\\mathbf{x})||_2^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f92dec2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\nabla J(\\mathbf{x}) = \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}^\\text{T} \\mathbf{g}(\\mathbf{x})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\nabla^2 J(\\mathbf{x}) = \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}^\\text{T} \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}} + \\nabla(\\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}^\\text{T}) \\mathbf{g}(\\mathbf{x}) \\approx \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}^\\text{T} \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\Delta \\mathbf{x} = - \\left(\\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}^\\text{T} \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}\\right)^{-1}\n",
    "    \\frac{\\partial \\mathbf{g}(\\mathbf{x})}{\\partial \\mathbf{x}}^\\text{T} \\mathbf{g}(\\mathbf{x})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458fc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = Figure()\n",
    "ax = Axis(fig[1,1], aspect=1)\n",
    "\n",
    "## Initial guess\n",
    "# xᵢ = Float64[-0.75; -1.75]\n",
    "xᵢ = Float64[-3; 2]\n",
    "λᵢ = Float64[0.0]\n",
    "\n",
    "x2ᵢ, λ2ᵢ = xᵢ, λᵢ\n",
    "\n",
    "draw_contour(ax)\n",
    "plot!(ax, [xᵢ[1]], [xᵢ[2]], color=:green, marker=:circle, markersize=15)\n",
    "# plot!(ax, [xᵢ[1]], [xᵢ[2]], color=:red, marker=:circle, markersize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x2ᵢ₊₁, λ2ᵢ₊₁ = newton_step(x2ᵢ, λ2ᵢ)\n",
    "xᵢ₊₁, λᵢ₊₁ = gauss_newton_step(xᵢ, λᵢ)\n",
    "plot!(ax, [xᵢ₊₁[1]], [xᵢ₊₁[2]], color=:green, marker=:x, markersize=15)\n",
    "# plot!(ax, [x2ᵢ₊₁[1]], [x2ᵢ₊₁[2]], color=:red, marker=:x, markersize=15)\n",
    "xᵢ .= xᵢ₊₁\n",
    "λᵢ .= λᵢ₊₁\n",
    "# x2ᵢ .= x2ᵢ₊₁\n",
    "# λ2ᵢ .= λ2ᵢ₊₁\n",
    "\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4cddda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
